#!/usr/bin/env bun
/**
 * Codex Exploit Proof Gate Script
 *
 * For each HIGH/MED finding that was patched, Codex writes Foundry exploit
 * PoCs. Then attempts to find NEW exploits in patched code.
 * If exploit succeeds against patched code -> fix insufficient -> back to red-team.
 *
 * EVMbench evidence: Codex scores 72.2% on Exploit (best of all models).
 *
 * Usage:
 *   bun codex-exploit-verify.js --run-id <run_id> [--timeout 1200000]
 *
 * Environment:
 *   CLAUDE_PROJECT_DIR - Project directory
 *   CLAUDE_PLUGIN_ROOT - Plugin installation directory
 *   CODEX_API_KEY - Codex API key (optional, uses default if not set)
 */

import { readFileSync, writeFileSync, existsSync, mkdirSync, readdirSync } from 'fs';
import { join, dirname } from 'path';
import { parseArgs } from 'util';
import { execSync, spawn } from 'child_process';

const PROJECT_DIR = process.env.CLAUDE_PROJECT_DIR || process.cwd();
const PLUGIN_ROOT = process.env.CLAUDE_PLUGIN_ROOT || dirname(dirname(import.meta.path));
const TASK_DIR = join(PROJECT_DIR, '.task');
const DOCS_DIR = join(PROJECT_DIR, 'docs');

function loadCodexStageConfig(stageKey) {
  try {
    const configPath = join(PROJECT_DIR, '.claude-codex.json');
    if (!existsSync(configPath)) return null;
    const config = JSON.parse(readFileSync(configPath, 'utf8'));
    return config?.codex_stages?.[stageKey] ?? null;
  } catch {
    return null;
  }
}

function loadExploitVerificationConfig() {
  try {
    const configPath = join(PROJECT_DIR, '.claude-codex.json');
    if (!existsSync(configPath)) return {};
    const config = JSON.parse(readFileSync(configPath, 'utf8'));
    return config?.exploit_verification ?? {};
  } catch {
    return {};
  }
}

function writeExecutionLog(stage, data) {
  try {
    const logsDir = join(PROJECT_DIR, 'reports', 'execution-logs');
    mkdirSync(logsDir, { recursive: true });
    const timestamp = new Date().toISOString().replace(/[:.]/g, '-');
    const logFile = join(logsDir, `${stage}-${timestamp}.log`);
    const content = Object.entries(data)
      .map(([k, v]) => `${k}: ${v}`)
      .join('\n') + '\n';
    writeFileSync(logFile, content);
  } catch {
    // Non-critical, don't fail
  }
}

/**
 * Parse token usage from Codex CLI output (G9)
 */
function parseTokenUsage(stdout, stderr) {
  const combined = (stdout || '') + '\n' + (stderr || '');
  const usage = { input_tokens: 0, output_tokens: 0, total_tokens: 0 };
  let found = false;
  const nestedPattern = /"usage"\s*:\s*\{([^}]+)\}/g;
  let match;
  while ((match = nestedPattern.exec(combined)) !== null) {
    try {
      const usageObj = JSON.parse(`{${match[1]}}`);
      usage.input_tokens += usageObj.input_tokens || usageObj.prompt_tokens || 0;
      usage.output_tokens += usageObj.output_tokens || usageObj.completion_tokens || 0;
      found = true;
    } catch { /* skip */ }
  }
  const totalMatch = combined.match(/total[_ ]tokens?\s*[:=]\s*(\d+)/i);
  if (totalMatch && !found) {
    usage.total_tokens = parseInt(totalMatch[1]);
    found = true;
  }
  if (found) {
    usage.total_tokens = usage.total_tokens || (usage.input_tokens + usage.output_tokens);
    return usage;
  }
  return null;
}

function parseArguments() {
  const { values } = parseArgs({
    options: {
      'run-id': { type: 'string' },
      'findings-path': { type: 'string' },
      'timeout': { type: 'string' },
      'live-chain': { type: 'boolean' },
      help: { type: 'boolean', short: 'h' }
    },
    allowPositionals: true
  });

  if (values.help) {
    console.log(`
Usage: codex-exploit-verify.js --run-id <run_id> [options]

Codex Exploit Proof Gate: writes Foundry exploit PoCs to verify fixes.
If exploit succeeds against patched code -> fix insufficient -> back to red-team.

Options:
  --run-id          Run ID for this pipeline execution
  --findings-path   Path to findings JSON (default: auto-detect from .task/<run_id>)
  --timeout         Timeout in milliseconds (default: 1200000 = 20 minutes)
  --live-chain      Use live chain mode (Anvil + RPC gatekeeper) instead of Foundry tests
  -h, --help        Show this help message
    `);
    process.exit(0);
  }

  return values;
}

function readFile(filePath) {
  try {
    if (!existsSync(filePath)) return null;
    return readFileSync(filePath, 'utf-8');
  } catch {
    return null;
  }
}

function ensureDir(dir) {
  if (!existsSync(dir)) {
    mkdirSync(dir, { recursive: true });
  }
}

/**
 * Find findings JSON from various possible locations
 */
function findFindings(runId, explicitPath) {
  if (explicitPath && existsSync(explicitPath)) {
    return JSON.parse(readFileSync(explicitPath, 'utf-8'));
  }

  // Check common locations for findings
  const candidates = [
    join(TASK_DIR, runId, 'redteam-issue-log.json'),
    join(TASK_DIR, runId, 'codex-detect-findings.json'),
    join(TASK_DIR, runId, 'opus-detect-findings.json'),
    join(TASK_DIR, 'redteam-issue-log.json'),
    join(DOCS_DIR, 'reviews', 'codex-detect-findings.json'),
  ];

  for (const candidate of candidates) {
    if (existsSync(candidate)) {
      try {
        const data = JSON.parse(readFileSync(candidate, 'utf-8'));
        console.log(`Found findings at: ${candidate}`);
        return data;
      } catch {
        continue;
      }
    }
  }

  return null;
}

/**
 * Extract HIGH/MED findings that need verification
 */
function extractVerifiableFindings(findingsData) {
  const findings = findingsData?.findings || findingsData?.issues || [];
  return findings.filter(f => {
    const severity = (f.severity || '').toUpperCase();
    return severity === 'HIGH' || severity === 'MEDIUM' || severity === 'MED';
  });
}

/**
 * Collect git diffs for patches (if available)
 */
function collectPatches(runId) {
  const patchDir = join(TASK_DIR, runId, 'patches');
  if (!existsSync(patchDir)) return '(No patch diffs available)';

  try {
    const files = readdirSync(patchDir).filter(f => f.endsWith('.diff') || f.endsWith('.patch'));
    if (files.length === 0) return '(No patch diffs found)';

    return files.map(f => {
      const content = readFileSync(join(patchDir, f), 'utf-8');
      return `### ${f}\n\`\`\`diff\n${content}\n\`\`\``;
    }).join('\n\n');
  } catch {
    return '(Error reading patches)';
  }
}

/**
 * Write INSTRUCTIONS.md, findings.json, and patches.md to workspace for exec mode (G6).
 * Returns the workspace path.
 */
function writeWorkspace(findings, patches, runId, config) {
  const maxAttempts = config.max_exploit_attempts_per_finding || 3;
  const workspace = join(TASK_DIR, runId, 'exploit-verify-workspace');
  ensureDir(workspace);

  // Write findings.json to workspace
  writeFileSync(join(workspace, 'findings.json'), JSON.stringify(findings, null, 2));

  // Write patches.md to workspace
  writeFileSync(join(workspace, 'patches.md'), patches);

  const instructions = `# Codex Exploit Proof Gate — INSTRUCTIONS

You are a Codex Exploit Prover performing the Exploit Proof Gate for a fund-sensitive smart contract audit.

## YOUR MISSION

For each HIGH/MED finding in \`${join(workspace, 'findings.json')}\`, you MUST:
1. Write a Foundry test that exploits the ORIGINAL vulnerability (proves you understand it)
2. Write up to ${maxAttempts} exploit attempts against the PATCHED code
3. If ANY exploit succeeds against patched code -> verdict is PATCH_INSUFFICIENT
4. If ALL exploits fail against patched code -> verdict is PATCH_VALID

## INPUT FILES

- \`${join(workspace, 'findings.json')}\` — Findings to verify (HIGH/MED severity)
- \`${join(workspace, 'patches.md')}\` — Patch diffs applied to fix findings

## INSTRUCTIONS

### Step 1: Read findings and patches
Read the findings and patches from the workspace files above.

### Step 2: Create exploit proof test files

For each finding, create: \`test/exploit-proofs/ExploitProof_{ID}.t.sol\`

Each test file should contain:
- \`test_exploit_{ID}_original()\` — Exploits the original vulnerability
- \`test_exploit_{ID}_patched_attempt1()\` — Tries original exploit against patched code
- \`test_exploit_{ID}_patched_attempt2()\` — Tries variant exploit
- \`test_exploit_{ID}_patched_attempt3()\` — Tries different attack vector

### Step 3: Run forge test

\`\`\`bash
forge test --match-path "test/exploit-proofs/" -vvv
\`\`\`

### Step 4: Analyze results and write report

For each finding, determine:
- Did the original exploit reproduce? (validates understanding)
- Did ANY patched exploit succeed? (PATCH_INSUFFICIENT if yes)

### Step 5: Write output files

Write both files to the workspace directory:

1. **\`${join(workspace, 'codex-exploit-proof.md')}\`** — Human-readable report
2. **\`${join(workspace, 'codex-exploit-proof.json')}\`** — Machine-readable artifact

The JSON artifact MUST follow this schema:
\`\`\`json
{
  "id": "codex-exploit-proof-{timestamp}",
  "reviewer": "codex-exploit-prover",
  "model": "codex",
  "findings_verified": [
    {
      "finding_id": "VULN-1",
      "severity": "HIGH",
      "original_exploit_reproduced": true,
      "patched_attempts": ${maxAttempts},
      "patched_exploit_succeeded": false,
      "verdict": "PATCH_VALID",
      "test_file": "test/exploit-proofs/ExploitProof_VULN1.t.sol",
      "forge_output": "..."
    }
  ],
  "overall_verdict": "ALL_PATCHES_VALID",
  "insufficient_patches": [],
  "forge_test_command": "forge test --match-path test/exploit-proofs/ -vvv",
  "forge_test_output": "...",
  "generated_at": "..."
}
\`\`\`

## QUALITY CRITERIA

- Every PoC MUST be a runnable Foundry test
- Flash loan scenarios MUST be attempted where relevant
- Multi-step exploits spanning multiple transactions MUST be tested
- At least ${maxAttempts} exploit attempts per finding against patched code
- PATCH_VALID verdict requires evidence that all attempts failed

## BEGIN

Read the source files, understand each finding, write exploit proofs, run them, and report results.
`;

  writeFileSync(join(workspace, 'INSTRUCTIONS.md'), instructions);
  return workspace;
}

/**
 * Invoke Codex CLI in exec mode (G6).
 * cwd is PROJECT_DIR (needs forge access). Workspace INSTRUCTIONS.md referenced by absolute path.
 */
async function invokeCodex(workspace, timeout) {
  return new Promise((resolve, reject) => {
    const codexPath = process.env.CODEX_PATH || 'codex';

    console.log('Invoking Codex CLI (exec mode) for exploit proof gate...');
    console.log(`Timeout: ${timeout}ms (${Math.round(timeout / 60000)} minutes)`);

    const stageConfig = loadCodexStageConfig('exploit');

    const args = [
      'exec',
      '--full-auto',
      '--skip-git-repo-check'
    ];

    if (stageConfig?.model) {
      args.push('-m', stageConfig.model);
    }
    if (stageConfig?.reasoning) {
      args.push('-c', `model_reasoning_effort="${stageConfig.reasoning}"`);
    }

    const instructionsPath = join(workspace, 'INSTRUCTIONS.md');
    args.push(`Read ${instructionsPath} and perform exploit proof verification. Write results to the workspace directory specified in INSTRUCTIONS.md.`);

    const child = spawn(codexPath, args, {
      cwd: PROJECT_DIR,
      env: {
        ...process.env,
        CLAUDE_PROJECT_DIR: PROJECT_DIR
      },
      stdio: ['pipe', 'pipe', 'pipe'],
      timeout
    });

    let stdout = '';
    let stderr = '';

    child.stdout.on('data', (data) => {
      stdout += data.toString();
      process.stdout.write(data);
    });

    child.stderr.on('data', (data) => {
      stderr += data.toString();
      process.stderr.write(data);
    });

    const timeoutId = setTimeout(() => {
      child.kill('SIGTERM');
      reject(new Error(`Codex timed out after ${timeout}ms`));
    }, timeout);

    child.on('close', (code) => {
      clearTimeout(timeoutId);
      if (code === 0) {
        resolve({ stdout, stderr, code });
      } else {
        reject(new Error(`Codex exited with code ${code}\n${stderr}`));
      }
    });

    child.on('error', (err) => {
      clearTimeout(timeoutId);
      reject(err);
    });
  });
}

/**
 * Create fallback output if Codex fails
 */
function createFallbackOutput(runId, error, findingsCount) {
  const reviewsDir = join(DOCS_DIR, 'reviews');
  ensureDir(reviewsDir);

  const timestamp = new Date().toISOString();
  const dateStr = timestamp.replace(/[-:]/g, '').split('.')[0].replace('T', '-');

  const mdContent = `# Codex Exploit Proof Gate

**Reviewer:** codex-exploit-prover
**Model:** codex (FAILED)
**Date:** ${timestamp}

## Error

Codex CLI invocation failed:

\`\`\`
${error.message}
\`\`\`

## Action Required

${findingsCount} HIGH/MED findings need exploit verification.

Please run exploit proof gate manually:

\`\`\`bash
bun "${PLUGIN_ROOT}/scripts/codex-exploit-verify.js" --run-id ${runId}
\`\`\`
`;

  writeFileSync(join(reviewsDir, 'codex-exploit-proof.md'), mdContent);

  const jsonContent = {
    id: `codex-exploit-proof-${dateStr}`,
    reviewer: 'codex-exploit-prover',
    model: 'codex',
    error: error.message,
    status: 'FAILED',
    findings_verified: [],
    overall_verdict: 'FAILED',
    insufficient_patches: [],
    generated_at: timestamp
  };

  const runDir = join(TASK_DIR, runId);
  ensureDir(runDir);
  writeFileSync(join(runDir, 'codex-exploit-proof.json'), JSON.stringify(jsonContent, null, 2));

  return { md: mdContent, json: jsonContent };
}

/**
 * Run live chain exploit mode (G4).
 * Starts Anvil + gatekeeper, runs Codex with live-exploit-prover prompt, grades results.
 */
async function runLiveChainMode(runId, verifiableFindings, timeout, config) {
  const { execSync: execSyncLocal } = await import('child_process');
  const liveConfig = config.live_chain || {};
  const anvilPort = liveConfig.anvil_port || 8545;
  const gatekeeperPort = liveConfig.gatekeeper_port || 8546;
  const seedEth = liveConfig.seed_eth || '100';
  const liveTimeout = liveConfig.timeout_ms || timeout;

  console.log(`\n--- Live Chain Mode ---`);
  console.log(`Anvil port: ${anvilPort}, Gatekeeper port: ${gatekeeperPort}`);

  const runDir = join(TASK_DIR, runId);
  ensureDir(runDir);

  // Write findings for Codex to read
  writeFileSync(join(runDir, 'live-chain-findings.json'), JSON.stringify(verifiableFindings, null, 2));

  let envProcess = null;

  try {
    // 1. Start exploit environment in background
    console.log('Starting exploit environment...');
    const envScript = join(PLUGIN_ROOT, 'scripts', 'run-exploit-env.js');
    envProcess = spawn('bun', [
      envScript,
      '--run-id', runId,
      '--anvil-port', String(anvilPort),
      '--gatekeeper-port', String(gatekeeperPort),
      '--seed-eth', seedEth,
      '--timeout', String(liveTimeout + 60000) // extra minute buffer
    ], {
      cwd: PROJECT_DIR,
      env: { ...process.env, CLAUDE_PROJECT_DIR: PROJECT_DIR, CLAUDE_PLUGIN_ROOT: PLUGIN_ROOT },
      stdio: ['pipe', 'pipe', 'pipe']
    });

    // Wait for deploy-artifacts.json to appear
    const artifactsPath = join(runDir, 'deploy-artifacts.json');
    const waitStart = Date.now();
    while (!existsSync(artifactsPath) && (Date.now() - waitStart) < 30000) {
      await new Promise(r => setTimeout(r, 500));
    }

    if (!existsSync(artifactsPath)) {
      throw new Error('Exploit environment failed to start (no deploy-artifacts.json after 30s)');
    }

    console.log('Exploit environment ready');

    // 2. Create workspace with INSTRUCTIONS.md for live-chain mode
    const workspace = join(runDir, 'live-exploit-workspace');
    ensureDir(workspace);

    // Copy artifacts and findings to workspace
    writeFileSync(join(workspace, 'deploy-artifacts.json'), readFileSync(artifactsPath, 'utf-8'));
    writeFileSync(join(workspace, 'findings.json'), JSON.stringify(verifiableFindings, null, 2));

    const liveInstructions = `# Live Chain Exploit Verification — INSTRUCTIONS

Read the codex-live-exploit-prover agent prompt for your full instructions.

## INPUT FILES
- \`${join(workspace, 'deploy-artifacts.json')}\` — RPC URL, attacker key, contract addresses
- \`${join(workspace, 'findings.json')}\` — Findings to exploit

## OUTPUT FILES
Write to this workspace:
- \`${join(workspace, 'live-exploit-proof.json')}\` — Machine-readable results
- \`${join(workspace, 'live-exploit-proof.md')}\` — Human-readable report

## KEY RULES
1. Use \`cast send\` and \`cast call\` for transactions — NOT Foundry tests
2. RPC goes through gatekeeper: only eth_*/net_*/web3_* methods allowed
3. Try at least 3 exploit vectors per finding
4. Document every transaction hash
`;

    writeFileSync(join(workspace, 'INSTRUCTIONS.md'), liveInstructions);

    // 3. Invoke Codex in exec mode
    const codexPath = process.env.CODEX_PATH || 'codex';
    const stageConfig = loadCodexStageConfig('exploit');

    const codexArgs = [
      'exec',
      '--full-auto',
      '--skip-git-repo-check'
    ];
    if (stageConfig?.model) codexArgs.push('-m', stageConfig.model);
    if (stageConfig?.reasoning) codexArgs.push('-c', `model_reasoning_effort="${stageConfig.reasoning}"`);
    codexArgs.push(`Read ${join(workspace, 'INSTRUCTIONS.md')} and perform live chain exploit verification. Send real transactions using cast CLI. Write results to the workspace.`);

    console.log('Invoking Codex for live chain exploit...');
    const codexResult = await new Promise((resolve, reject) => {
      const child = spawn(codexPath, codexArgs, {
        cwd: PROJECT_DIR,
        env: { ...process.env, CLAUDE_PROJECT_DIR: PROJECT_DIR },
        stdio: ['pipe', 'pipe', 'pipe'],
        timeout: liveTimeout
      });

      let stdout = '', stderr = '';
      child.stdout.on('data', d => { stdout += d; process.stdout.write(d); });
      child.stderr.on('data', d => { stderr += d; process.stderr.write(d); });

      const tid = setTimeout(() => { child.kill('SIGTERM'); reject(new Error('Codex timed out')); }, liveTimeout);
      child.on('close', code => { clearTimeout(tid); code === 0 ? resolve({ stdout, stderr }) : reject(new Error(`Codex exited ${code}`)); });
      child.on('error', err => { clearTimeout(tid); reject(err); });
    });

    // 4. Grade exploit results — replay-isolated (EVMbench grading container pattern)
    console.log('\nGrading exploit results (replay-isolated)...');
    const { gradeExploit } = await import('./grade-exploit.js');

    // Read deploy artifacts and grading secrets for replay
    const artifacts = JSON.parse(readFileSync(artifactsPath, 'utf-8'));
    const secretsPath = join(runDir, 'grading-secrets.json');
    const secrets = existsSync(secretsPath)
      ? JSON.parse(readFileSync(secretsPath, 'utf-8'))
      : {};
    const auditTrailPath = join(runDir, 'rpc-audit-trail.jsonl');
    let gradeResult;

    if (existsSync(auditTrailPath) && secrets.mnemonic) {
      // Replay-isolated grading: start fresh Anvil, replay txs, grade fresh state
      console.log('  Replaying transactions on fresh chain for isolated grading...');
      try {
        const { replayTransactions } = await import('./replay-transactions.js');
        const { startAnvil } = await import('./run-exploit-env.js');

        // Read recorded transactions from audit trail
        const txLines = readFileSync(auditTrailPath, 'utf-8').trim().split('\n').filter(Boolean);
        const txRecords = [];
        for (const line of txLines) {
          try {
            const entry = JSON.parse(line);
            if (entry.tx_params && entry.allowed) {
              txRecords.push({
                sequence: txRecords.length,
                method: entry.method || 'eth_sendRawTransaction',
                params: entry.tx_params,
                tx_hash: entry.tx_hash
              });
            }
          } catch { /* skip malformed lines */ }
        }

        if (txRecords.length > 0) {
          // Start fresh Anvil with same mnemonic for identical address derivation
          const freshPort = (secrets.anvil_port || artifacts.anvil_port || 8545) + 100;
          const freshAnvil = await startAnvil(freshPort, secrets.mnemonic);

          try {
            const freshRpc = `http://localhost:${freshPort}`;
            // Replay all recorded transactions on fresh chain
            const replayResult = await replayTransactions(freshRpc, txRecords);
            console.log(`  Replay: ${replayResult.summary.succeeded}/${replayResult.summary.total} succeeded`);

            // Grade on fresh chain state
            gradeResult = await gradeExploit(artifactsPath, freshRpc);
            gradeResult.grading_mode = 'replay-isolated';
            gradeResult.replay_summary = replayResult.summary;
          } finally {
            try { freshAnvil.process.kill('SIGTERM'); } catch {}
          }
        } else {
          console.log('  No transactions recorded; falling back to direct grading');
          gradeResult = await gradeExploit(artifactsPath);
          gradeResult.grading_mode = 'direct-no-txs';
        }
      } catch (replayErr) {
        console.warn(`  Replay grading failed (${replayErr.message}); falling back to direct grading`);
        gradeResult = await gradeExploit(artifactsPath);
        gradeResult.grading_mode = 'direct-fallback';
      }
    } else {
      // No audit trail: grade directly (pre-gatekeeper or gatekeeper disabled)
      gradeResult = await gradeExploit(artifactsPath);
      gradeResult.grading_mode = 'direct';
    }

    console.log(`Grade verdict: ${gradeResult.verdict} (mode: ${gradeResult.grading_mode})`);

    // 5. Copy results to canonical locations
    const wsJsonPath = join(workspace, 'live-exploit-proof.json');
    const wsMdPath = join(workspace, 'live-exploit-proof.md');
    const jsonPath = join(runDir, 'codex-exploit-proof.json');
    const mdPath = join(DOCS_DIR, 'reviews', 'codex-exploit-proof.md');

    // Merge Codex output with grade result
    let exploitProof = {};
    if (existsSync(wsJsonPath)) {
      exploitProof = JSON.parse(readFileSync(wsJsonPath, 'utf-8'));
    }
    exploitProof.live_chain_grade = gradeResult;
    exploitProof.mode = 'live-chain';

    writeFileSync(jsonPath, JSON.stringify(exploitProof, null, 2));
    if (existsSync(wsMdPath)) {
      ensureDir(join(DOCS_DIR, 'reviews'));
      writeFileSync(mdPath, readFileSync(wsMdPath, 'utf-8'));
    }

    // Write exploit-replay.json for hook validator (validateExploitReplay)
    const findings = exploitProof.findings || exploitProof.exploits_confirmed || [];
    const replayArtifact = {
      replays: findings.map(f => ({
        finding_id: f.id || f.finding_id || 'unknown',
        verdict: gradeResult.verdict,
        grading_mode: gradeResult.grading_mode,
        pre_balance: String(gradeResult.balances?.attacker?.eth_initial ?? '0'),
        post_balance: String(gradeResult.balances?.attacker?.eth_current ?? '0')
      })),
      grade_summary: gradeResult
    };
    // If no per-finding breakdown, write a single replay entry from grade result
    if (replayArtifact.replays.length === 0) {
      replayArtifact.replays = [{
        finding_id: 'aggregate',
        verdict: gradeResult.verdict,
        grading_mode: gradeResult.grading_mode
      }];
    }
    writeFileSync(join(TASK_DIR, 'exploit-replay.json'), JSON.stringify(replayArtifact, null, 2));
    console.log(`Exploit replay artifact: ${join(TASK_DIR, 'exploit-replay.json')}`);

    console.log(`\nLive chain exploit verification complete`);
    console.log(`Grade: ${gradeResult.verdict}`);
    console.log(JSON.stringify({ success: true, run_id: runId, mode: 'live-chain', grade: gradeResult.verdict }));

  } finally {
    // Cleanup: kill exploit environment
    if (envProcess) {
      try { envProcess.kill('SIGTERM'); } catch {}
      console.log('Exploit environment stopped');
    }
  }
}

async function main() {
  const args = parseArguments();
  const runId = args['run-id'] || `exploit-verify-${Date.now()}`;
  const timeout = parseInt(args.timeout || '1200000'); // 20 minutes default
  const config = loadExploitVerificationConfig();
  const useLiveChain = args['live-chain'] || config.live_chain?.enable;

  console.log(`\n=== Codex Exploit Proof Gate ===`);
  console.log(`Run ID: ${runId}`);
  console.log(`Timeout: ${timeout}ms`);
  console.log(`Mode: ${useLiveChain ? 'LIVE CHAIN' : 'Foundry tests'}`);
  console.log(`Max attempts per finding: ${config.max_exploit_attempts_per_finding || 3}`);

  // Find and load findings
  const findingsData = findFindings(runId, args['findings-path']);
  if (!findingsData) {
    console.error('No findings found. Nothing to verify.');
    console.log(JSON.stringify({ success: true, run_id: runId, findings_count: 0, message: 'No findings to verify' }));
    process.exit(0);
  }

  const verifiableFindings = extractVerifiableFindings(findingsData);
  console.log(`HIGH/MED findings to verify: ${verifiableFindings.length}`);

  if (verifiableFindings.length === 0) {
    console.log('No HIGH/MED findings to verify. Exploit proof gate passed by default.');
    const timestamp = new Date().toISOString();
    const runDir = join(TASK_DIR, runId);
    ensureDir(runDir);
    writeFileSync(join(runDir, 'codex-exploit-proof.json'), JSON.stringify({
      id: `codex-exploit-proof-${timestamp.replace(/[-:]/g, '').split('.')[0].replace('T', '-')}`,
      reviewer: 'codex-exploit-prover',
      model: 'codex',
      findings_verified: [],
      overall_verdict: 'ALL_PATCHES_VALID',
      insufficient_patches: [],
      generated_at: timestamp
    }, null, 2));
    console.log(JSON.stringify({ success: true, run_id: runId, findings_count: 0, verdict: 'ALL_PATCHES_VALID' }));
    process.exit(0);
  }

  // Live chain mode (G4): use Anvil + RPC gatekeeper instead of Foundry tests
  if (useLiveChain) {
    try {
      await runLiveChainMode(runId, verifiableFindings, timeout, config);
    } catch (err) {
      console.error('Live chain mode failed:', err.message);
      createFallbackOutput(runId, err, verifiableFindings.length);
      process.exit(1);
    }
    return;
  }

  // Standard Foundry test mode
  // Collect patches
  const patches = collectPatches(runId);

  // Write workspace with INSTRUCTIONS.md, findings.json, patches.md (G6: exec mode)
  const workspace = writeWorkspace(verifiableFindings, patches, runId, config);

  const startTime = Date.now();

  try {
    const result = await invokeCodex(workspace, timeout);
    console.log('\nCodex exploit proof gate completed successfully');

    // Copy outputs from workspace to canonical locations
    const wsJsonPath = join(workspace, 'codex-exploit-proof.json');
    const wsMdPath = join(workspace, 'codex-exploit-proof.md');
    const jsonPath = join(TASK_DIR, runId, 'codex-exploit-proof.json');
    const mdPath = join(DOCS_DIR, 'reviews', 'codex-exploit-proof.md');

    if (existsSync(wsJsonPath)) {
      writeFileSync(jsonPath, readFileSync(wsJsonPath, 'utf-8'));
    }
    if (existsSync(wsMdPath)) {
      ensureDir(join(DOCS_DIR, 'reviews'));
      writeFileSync(mdPath, readFileSync(wsMdPath, 'utf-8'));
    }

    if (existsSync(jsonPath)) {
      console.log(`JSON artifact: ${jsonPath}`);
      try {
        const proofResult = JSON.parse(readFileSync(jsonPath, 'utf-8'));
        const verdict = proofResult.overall_verdict || 'UNKNOWN';
        const insufficient = proofResult.insufficient_patches || [];
        console.log(`Overall verdict: ${verdict}`);
        if (insufficient.length > 0) {
          console.log(`INSUFFICIENT PATCHES: ${insufficient.join(', ')}`);
        }
      } catch { /* ignore parse error */ }
    } else {
      console.warn('Warning: JSON artifact not created by Codex');
    }

    if (existsSync(mdPath)) {
      console.log(`MD report: ${mdPath}`);
    } else {
      console.warn('Warning: MD report not created by Codex');
    }

    const durationMs = Date.now() - startTime;
    const exploitStageConfig = loadCodexStageConfig('exploit');
    const tokenUsage = parseTokenUsage(result.stdout, result.stderr);
    const logData = {
      start_time: new Date(startTime).toISOString(),
      model: exploitStageConfig?.model || 'default',
      reasoning: exploitStageConfig?.reasoning || 'default',
      timeout_ms: timeout,
      exit_code: 0,
      duration_ms: durationMs,
      run_id: runId,
      findings_count: verifiableFindings.length
    };
    if (tokenUsage) {
      logData.input_tokens = tokenUsage.input_tokens;
      logData.output_tokens = tokenUsage.output_tokens;
      logData.total_tokens = tokenUsage.total_tokens;
    }
    writeExecutionLog('codex-exploit-verify', logData);

    console.log(JSON.stringify({
      success: true,
      run_id: runId,
      json_path: jsonPath,
      md_path: mdPath,
      findings_verified: verifiableFindings.length
    }));
  } catch (error) {
    console.error('\nCodex exploit proof gate failed:', error.message);

    createFallbackOutput(runId, error, verifiableFindings.length);
    console.log('Created fallback output for manual retry');

    const durationMs = Date.now() - startTime;
    const exploitStageConfig = loadCodexStageConfig('exploit');
    writeExecutionLog('codex-exploit-verify', {
      start_time: new Date(startTime).toISOString(),
      model: exploitStageConfig?.model || 'default',
      reasoning: exploitStageConfig?.reasoning || 'default',
      timeout_ms: timeout,
      exit_code: 1,
      duration_ms: durationMs,
      run_id: runId,
      error: error.message
    });

    console.log(JSON.stringify({
      success: false,
      run_id: runId,
      error: error.message,
      fallback_created: true
    }));

    process.exit(1);
  }
}

main().catch(err => {
  console.error('Error:', err.message);
  process.exit(1);
});
